{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:\n",
      "0.4.1.post2\n",
      "GPU Detected:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from util import get_num_lines, get_vocab, embed_sequence, get_word2idx_idx2word, get_embedding_matrix\n",
    "from util import TextDatasetWithGloveElmoSuffix as TextDataset\n",
    "from util import evaluate\n",
    "from model import RNNSequenceClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import csv\n",
    "import h5py\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')  # to avoid the error: _tkinter.TclError: no display name and no $DISPLAY environment variable\n",
    "# matplotlib.use('tkagg') # to display the graph on remote server\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\")\n",
    "print(torch.__version__)\n",
    "print(\"GPU Detected:\")\n",
    "print(torch.cuda.is_available())\n",
    "using_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "1. Data pre-processing\n",
    "\"\"\"\n",
    "'''\n",
    "1.1 \n",
    "get rev_id --> label as a dictionary:\n",
    "    rev_id: string to indicate the id of the comment\n",
    "    label: int 1 or 0\n",
    "'''\n",
    "id2label = {}\n",
    "with open('../aggression_dataset/aggression_annotations.tsv') as f:\n",
    "    lines = csv.reader(f, delimiter='\\t')\n",
    "    next(lines, None)  # skip the headers\n",
    "    for line in lines:\n",
    "        rev_id = line[0]\n",
    "        # worker_id = line[1]\n",
    "        aggression_label = int(float(line[2]))\n",
    "        # aggression_score = float(line[3])\n",
    "        id2label[rev_id] = aggression_label                 \n",
    "'''\n",
    "1.2 \n",
    "get raw dataset as three list with given train/dev/test split:\n",
    "  Each element is a triple:\n",
    "    comment: the text needed to be classified (removed NEWLINE_TOKEN)\n",
    "    rev_id: string\n",
    "    label: int 1 or 0\n",
    "'''\n",
    "\n",
    "\n",
    "raw_train = []\n",
    "raw_dev = []\n",
    "raw_test = []\n",
    "with open('../aggression_dataset/aggression_annotated_comments.tsv') as f:\n",
    "    lines = csv.reader(f, delimiter='\\t')\n",
    "    next(lines, None)  # skip the headers\n",
    "    for line in lines:\n",
    "        rev_id = line[0]\n",
    "        # lowercse \n",
    "        comment = line[1].replace('NEWLINE_TOKEN', '').lower()\n",
    "        split = line[6]\n",
    "        sen_len = len(comment.split())\n",
    "        if  sen_len != 0 and sen_len < 10000:  # filter for length\n",
    "            if split == 'train':\n",
    "                raw_train.append([comment, rev_id, id2label[rev_id]])\n",
    "            elif split == 'dev':\n",
    "                raw_dev.append([comment,rev_id,  id2label[rev_id]])\n",
    "            else:\n",
    "                raw_test.append([comment, rev_id, id2label[rev_id]])     \n",
    "            \n",
    "# datset split without limit on sen_len: train, dev, test:  69526=12875+56648       23160       23178\n",
    "# with limit on sen_len <= 50: 44227=8710+35517    14663=939+11724      14493=2828+11665\n",
    "# with limit on sen_len <= 100: 57664=10868+46796          19186          19154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  152445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196017/2196017 [00:36<00:00, 60412.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors loaded:  43858\n",
      "Embeddings mean:  -0.007001449353992939\n",
      "Embeddings stdev:  0.3965314030647278\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Data preparation\n",
    "\"\"\"\n",
    "'''\n",
    "2. 1\n",
    "get vocabulary and glove embeddings in raw dataset \n",
    "'''\n",
    "# vocab is a set of words\n",
    "vocab = get_vocab(raw_train + raw_dev + raw_test)\n",
    "# two dictionaries. <PAD>: 0, <UNK>: 1\n",
    "word2idx, idx2word = get_word2idx_idx2word(vocab)\n",
    "# glove_embeddings a nn.Embeddings\n",
    "glove_embeddings = get_embedding_matrix(word2idx, idx2word, normalization=False)\n",
    "# elmo_embeddings\n",
    "# elmos_train_vua = h5py.File('../elmo/VUA_train.hdf5', 'r')\n",
    "# elmos_val_vua = h5py.File('../elmo/VUA_val.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset study\n",
    "\n",
    "# raw_train_pos = []  # 8710\n",
    "# raw_train_neg = []  # 35517\n",
    "# for example in raw_train:\n",
    "#     if example[2] == 1:\n",
    "#         raw_train_pos.append(example)\n",
    "#     else:\n",
    "#         raw_train_neg.append(example)\n",
    "# raw_train_balanced = raw_train_pos + raw_train_neg[:len(raw_train_pos)]\n",
    "# print(len(raw_train), len(raw_dev), len(raw_test))\n",
    "# print(len(raw_train_pos), len(raw_train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. 2\n",
    "embed the datasets\n",
    "'''\n",
    "\n",
    "\n",
    "elmos_train = None\n",
    "elmos_dev = None\n",
    "embedded_train = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_train), example[2]]\n",
    "                      for example in raw_train]\n",
    "embedded_dev = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_dev), example[2]]\n",
    "                    for example in raw_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. 3\n",
    "set up Dataloader for batching\n",
    "'''\n",
    "# Separate the input (embedded_sequence) and labels in the indexed train sets.\n",
    "train_dataset = TextDataset([example[0] for example in embedded_train],\n",
    "                                [example[1] for example in embedded_train])\n",
    "dev_dataset = TextDataset([example[0] for example in embedded_dev],\n",
    "                              [example[1] for example in embedded_dev])\n",
    "\n",
    "# Data-related hyperparameters\n",
    "batch_size = 256\n",
    "# Set up a DataLoader for the training, validation, and test dataset\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                  collate_fn=TextDataset.collate_fn)\n",
    "dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=batch_size,\n",
    "                                collate_fn=TextDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "total loss:  0.7033807635307312\n",
      "total loss:  0.7006012201309204\n",
      "total loss:  0.698373556137085\n",
      "total loss:  0.6956207752227783\n",
      "total loss:  0.6940983533859253\n",
      "total loss:  0.6907171607017517\n",
      "total loss:  0.6878277063369751\n",
      "total loss:  0.6867819428443909\n",
      "total loss:  0.6844305396080017\n",
      "total loss:  0.6806299090385437\n",
      "total loss:  0.6832157373428345\n",
      "total loss:  0.6766575574874878\n",
      "total loss:  0.6726333498954773\n",
      "total loss:  0.6747531890869141\n",
      "total loss:  0.6716988682746887\n",
      "total loss:  0.6648218631744385\n",
      "total loss:  0.6641291379928589\n",
      "total loss:  0.6611498594284058\n",
      "total loss:  0.6553524136543274\n",
      "total loss:  0.6594277024269104\n",
      "total loss:  0.6529259085655212\n",
      "total loss:  0.6552569270133972\n",
      "total loss:  0.6448972225189209\n",
      "total loss:  0.6467337012290955\n",
      "total loss:  0.6431085467338562\n",
      "total loss:  0.6455671787261963\n",
      "total loss:  0.6418192386627197\n",
      "total loss:  0.6384009718894958\n",
      "total loss:  0.6326025724411011\n",
      "total loss:  0.635673999786377\n",
      "total loss:  0.6362977623939514\n",
      "total loss:  0.6369513273239136\n",
      "total loss:  0.623293936252594\n",
      "total loss:  0.6194165349006653\n",
      "total loss:  0.6056250333786011\n",
      "total loss:  0.6192837357521057\n",
      "total loss:  0.6338325142860413\n",
      "total loss:  0.5979381203651428\n",
      "total loss:  0.5802696347236633\n",
      "total loss:  0.5990110039710999\n",
      "Starting epoch 2\n",
      "total loss:  0.5951340794563293\n",
      "total loss:  0.605914831161499\n",
      "total loss:  0.5838882923126221\n",
      "total loss:  0.598493218421936\n",
      "total loss:  0.6025774478912354\n",
      "total loss:  0.5816808938980103\n",
      "total loss:  0.5874341130256653\n",
      "total loss:  0.6021327376365662\n",
      "total loss:  0.5913267731666565\n",
      "total loss:  0.5862383246421814\n",
      "total loss:  0.5651060938835144\n",
      "total loss:  0.5736092925071716\n",
      "total loss:  0.5604814291000366\n",
      "total loss:  0.5649790167808533\n",
      "total loss:  0.5652174949645996\n",
      "total loss:  0.5430871248245239\n",
      "total loss:  0.5777438282966614\n",
      "total loss:  0.5702934265136719\n",
      "total loss:  0.5425211787223816\n",
      "total loss:  0.5870954394340515\n",
      "total loss:  0.5671026706695557\n",
      "total loss:  0.526709258556366\n",
      "total loss:  0.5292452573776245\n",
      "total loss:  0.5241525173187256\n",
      "total loss:  0.503993034362793\n",
      "total loss:  0.5411416292190552\n",
      "total loss:  0.5230169296264648\n",
      "total loss:  0.5301015973091125\n",
      "total loss:  0.5189136266708374\n",
      "total loss:  0.5651546716690063\n",
      "total loss:  0.5558745861053467\n",
      "total loss:  0.5179185271263123\n",
      "total loss:  0.5369176268577576\n",
      "total loss:  0.5457420349121094\n",
      "total loss:  0.5286051034927368\n",
      "total loss:  0.532460629940033\n",
      "total loss:  0.5504750609397888\n",
      "total loss:  0.4674057066440582\n",
      "total loss:  0.5886586308479309\n",
      "total loss:  0.6491129994392395\n",
      "Starting epoch 3\n",
      "total loss:  0.5410513281822205\n",
      "total loss:  0.5209776759147644\n",
      "total loss:  0.504023015499115\n",
      "total loss:  0.5553179383277893\n",
      "total loss:  0.5040543079376221\n",
      "total loss:  0.5040584206581116\n",
      "total loss:  0.5906709432601929\n",
      "total loss:  0.4674639403820038\n",
      "total loss:  0.5083494186401367\n",
      "total loss:  0.5165992975234985\n",
      "total loss:  0.5319055914878845\n",
      "total loss:  0.5439535975456238\n",
      "total loss:  0.5023987889289856\n",
      "total loss:  0.47884276509284973\n",
      "total loss:  0.5465575456619263\n",
      "total loss:  0.5701678395271301\n",
      "total loss:  0.533065140247345\n",
      "total loss:  0.47087275981903076\n",
      "total loss:  0.5222052335739136\n",
      "total loss:  0.5004643201828003\n",
      "total loss:  0.49536919593811035\n",
      "total loss:  0.5543019771575928\n",
      "total loss:  0.5040633082389832\n",
      "total loss:  0.5527018308639526\n",
      "total loss:  0.5274530649185181\n",
      "total loss:  0.5122331976890564\n",
      "total loss:  0.5294074416160583\n",
      "total loss:  0.4592346251010895\n",
      "total loss:  0.5397929549217224\n",
      "total loss:  0.5252933502197266\n",
      "total loss:  0.48350897431373596\n",
      "total loss:  0.521353006362915\n",
      "total loss:  0.504063606262207\n",
      "total loss:  0.4913193881511688\n",
      "total loss:  0.5257370471954346\n",
      "total loss:  0.508543848991394\n",
      "total loss:  0.566804051399231\n",
      "total loss:  0.49423283338546753\n",
      "total loss:  0.5134034156799316\n",
      "total loss:  0.36153149604797363\n",
      "Starting epoch 4\n",
      "total loss:  0.5170243978500366\n",
      "total loss:  0.5168185234069824\n",
      "total loss:  0.4986758828163147\n",
      "total loss:  0.4936771094799042\n",
      "total loss:  0.5268496870994568\n",
      "total loss:  0.49935227632522583\n",
      "total loss:  0.5673535466194153\n",
      "total loss:  0.5147212147712708\n",
      "total loss:  0.559526801109314\n",
      "total loss:  0.4549393355846405\n",
      "total loss:  0.49362751841545105\n",
      "total loss:  0.5173179507255554\n",
      "total loss:  0.4755345582962036\n",
      "total loss:  0.565818190574646\n",
      "total loss:  0.3847700357437134\n",
      "total loss:  0.5316328406333923\n",
      "total loss:  0.5074384212493896\n",
      "total loss:  0.4513445496559143\n",
      "total loss:  0.41702327132225037\n",
      "total loss:  0.5417644381523132\n",
      "total loss:  0.4716830253601074\n",
      "total loss:  0.5300910472869873\n",
      "total loss:  0.4804401993751526\n",
      "total loss:  0.5544060468673706\n",
      "total loss:  0.5105486512184143\n",
      "total loss:  0.5124051570892334\n",
      "total loss:  0.511178195476532\n",
      "total loss:  0.5489848256111145\n",
      "total loss:  0.5102018117904663\n",
      "total loss:  0.4462563097476959\n",
      "total loss:  0.5021721720695496\n",
      "total loss:  0.525888979434967\n",
      "total loss:  0.5226484537124634\n",
      "total loss:  0.5059856176376343\n",
      "total loss:  0.554917573928833\n",
      "total loss:  0.5230354070663452\n",
      "total loss:  0.50455641746521\n",
      "total loss:  0.4576977789402008\n",
      "total loss:  0.46366018056869507\n",
      "total loss:  0.6557170748710632\n",
      "Starting epoch 5\n",
      "total loss:  0.4525373876094818\n",
      "total loss:  0.5126547813415527\n",
      "total loss:  0.5211508870124817\n",
      "total loss:  0.4624924659729004\n",
      "total loss:  0.5085114240646362\n",
      "total loss:  0.516631543636322\n",
      "total loss:  0.5037811398506165\n",
      "total loss:  0.49892163276672363\n",
      "total loss:  0.4897911846637726\n",
      "total loss:  0.5290036201477051\n",
      "total loss:  0.4965319335460663\n",
      "total loss:  0.46802014112472534\n",
      "total loss:  0.514200747013092\n",
      "total loss:  0.5305600166320801\n",
      "total loss:  0.4765709340572357\n",
      "total loss:  0.5418216586112976\n",
      "total loss:  0.48164480924606323\n",
      "total loss:  0.5107448697090149\n",
      "total loss:  0.48567673563957214\n",
      "total loss:  0.45693856477737427\n",
      "total loss:  0.47218480706214905\n",
      "total loss:  0.49430087208747864\n",
      "total loss:  0.445366770029068\n",
      "total loss:  0.530984103679657\n",
      "total loss:  0.4723474979400635\n",
      "total loss:  0.5053215622901917\n",
      "total loss:  0.4597399830818176\n",
      "total loss:  0.5230353474617004\n",
      "total loss:  0.44192570447921753\n",
      "total loss:  0.5552845597267151\n",
      "total loss:  0.4761113226413727\n",
      "total loss:  0.5344298481941223\n",
      "total loss:  0.4979802668094635\n",
      "total loss:  0.46552056074142456\n",
      "total loss:  0.5171146988868713\n",
      "total loss:  0.4633878171443939\n",
      "total loss:  0.41661298274993896\n",
      "total loss:  0.45559898018836975\n",
      "total loss:  0.5073856711387634\n",
      "total loss:  0.4358661472797394\n",
      "[[420.  70.]\n",
      " [  8.   2.]]\n",
      "Iteration 200. Validation Loss 0.4178973138332367. Validation Accuracy 84. Validation Precision 20.0. Validation Recall 2.7777777777777777. Validation F1 4.878048780487805. Validation class-wise F1 48.190658377172.\n",
      "[[841. 147.]\n",
      " [  9.   3.]]\n",
      "Iteration 200. Training Loss 0.4195486903190613. Training Accuracy 84. Training Precision 25.0. Training Recall 2.0. Training F1 3.7037037037037037. Training class-wise F1 47.608108652722365.\n",
      "Starting epoch 6\n",
      "total loss:  0.4442485272884369\n",
      "total loss:  0.5527377128601074\n",
      "total loss:  0.44870349764823914\n",
      "total loss:  0.49897775053977966\n",
      "total loss:  0.607191801071167\n",
      "total loss:  0.45495641231536865\n",
      "total loss:  0.45957231521606445\n",
      "total loss:  0.47180724143981934\n",
      "total loss:  0.46886658668518066\n",
      "total loss:  0.4996809661388397\n",
      "total loss:  0.49015480279922485\n",
      "total loss:  0.5003213286399841\n",
      "total loss:  0.45649755001068115\n",
      "total loss:  0.5292572975158691\n",
      "total loss:  0.5007747411727905\n",
      "total loss:  0.45674532651901245\n",
      "total loss:  0.4491931200027466\n",
      "total loss:  0.4767266511917114\n",
      "total loss:  0.4930485785007477\n",
      "total loss:  0.4657938778400421\n",
      "total loss:  0.47052255272865295\n",
      "total loss:  0.4687948226928711\n",
      "total loss:  0.5123024582862854\n",
      "total loss:  0.5247678756713867\n",
      "total loss:  0.4982762038707733\n",
      "total loss:  0.46287277340888977\n",
      "total loss:  0.4256337583065033\n",
      "total loss:  0.45173782110214233\n",
      "total loss:  0.4267551898956299\n",
      "total loss:  0.44608259201049805\n",
      "total loss:  0.4432733952999115\n",
      "total loss:  0.4850543141365051\n",
      "total loss:  0.45494699478149414\n",
      "total loss:  0.4965526759624481\n",
      "total loss:  0.4806257486343384\n",
      "total loss:  0.48473742604255676\n",
      "total loss:  0.4418291449546814\n",
      "total loss:  0.4970981478691101\n",
      "total loss:  0.4399764835834503\n",
      "total loss:  0.29776179790496826\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  0.43095171451568604\n",
      "total loss:  0.42974433302879333\n",
      "total loss:  0.4526609480381012\n",
      "total loss:  0.4641832113265991\n",
      "total loss:  0.505105197429657\n",
      "total loss:  0.5218487977981567\n",
      "total loss:  0.525973379611969\n",
      "total loss:  0.4770985543727875\n",
      "total loss:  0.469107449054718\n",
      "total loss:  0.45030879974365234\n",
      "total loss:  0.4349500834941864\n",
      "total loss:  0.5540388822555542\n",
      "total loss:  0.4777229428291321\n",
      "total loss:  0.3999686539173126\n",
      "total loss:  0.43161818385124207\n",
      "total loss:  0.487554669380188\n",
      "total loss:  0.4215661585330963\n",
      "total loss:  0.40403446555137634\n",
      "total loss:  0.48537275195121765\n",
      "total loss:  0.4491118788719177\n",
      "total loss:  0.4936070740222931\n",
      "total loss:  0.4464767575263977\n",
      "total loss:  0.42852583527565\n",
      "total loss:  0.48870646953582764\n",
      "total loss:  0.4428735673427582\n",
      "total loss:  0.4640365540981293\n",
      "total loss:  0.4114541709423065\n",
      "total loss:  0.47916361689567566\n",
      "total loss:  0.39641085267066956\n",
      "total loss:  0.4368857741355896\n",
      "total loss:  0.48714709281921387\n",
      "total loss:  0.45248153805732727\n",
      "total loss:  0.5153393149375916\n",
      "total loss:  0.40755876898765564\n",
      "total loss:  0.4678732752799988\n",
      "total loss:  0.5154733657836914\n",
      "total loss:  0.4782937467098236\n",
      "total loss:  0.44622480869293213\n",
      "total loss:  0.3931688070297241\n",
      "total loss:  0.432317852973938\n",
      "Starting epoch 8\n",
      "total loss:  0.4499537944793701\n",
      "total loss:  0.4563663899898529\n",
      "total loss:  0.4441763162612915\n",
      "total loss:  0.4607427418231964\n",
      "total loss:  0.46860677003860474\n",
      "total loss:  0.4724617898464203\n",
      "total loss:  0.44701340794563293\n",
      "total loss:  0.42868393659591675\n",
      "total loss:  0.42980149388313293\n",
      "total loss:  0.3964698612689972\n",
      "total loss:  0.43223774433135986\n",
      "total loss:  0.4965607523918152\n",
      "total loss:  0.41285884380340576\n",
      "total loss:  0.4475698173046112\n",
      "total loss:  0.4235730767250061\n",
      "total loss:  0.36645886301994324\n",
      "total loss:  0.48771852254867554\n",
      "total loss:  0.4832603633403778\n",
      "total loss:  0.42321401834487915\n",
      "total loss:  0.41075292229652405\n",
      "total loss:  0.40030136704444885\n",
      "total loss:  0.4407617449760437\n",
      "total loss:  0.45266905426979065\n",
      "total loss:  0.36458462476730347\n",
      "total loss:  0.4631877839565277\n",
      "total loss:  0.3901766240596771\n",
      "total loss:  0.41511043906211853\n",
      "total loss:  0.4146113693714142\n",
      "total loss:  0.404922217130661\n",
      "total loss:  0.4880274534225464\n",
      "total loss:  0.4321517050266266\n",
      "total loss:  0.4954071640968323\n",
      "total loss:  0.4469042122364044\n",
      "total loss:  0.4051308333873749\n",
      "total loss:  0.40276017785072327\n",
      "total loss:  0.43649113178253174\n",
      "total loss:  0.4221797585487366\n",
      "total loss:  0.4014681279659271\n",
      "total loss:  0.502892792224884\n",
      "total loss:  0.3951219618320465\n",
      "Starting epoch 9\n",
      "total loss:  0.4136219024658203\n",
      "total loss:  0.42709818482398987\n",
      "total loss:  0.3906891942024231\n",
      "total loss:  0.40916407108306885\n",
      "total loss:  0.4574427902698517\n",
      "total loss:  0.4365135729312897\n",
      "total loss:  0.37488922476768494\n",
      "total loss:  0.4246475100517273\n",
      "total loss:  0.4035857021808624\n",
      "total loss:  0.45382291078567505\n",
      "total loss:  0.35641375184059143\n",
      "total loss:  0.4589807689189911\n",
      "total loss:  0.42542487382888794\n",
      "total loss:  0.38417014479637146\n",
      "total loss:  0.42606407403945923\n",
      "total loss:  0.4082971513271332\n",
      "total loss:  0.4462592899799347\n",
      "total loss:  0.41729483008384705\n",
      "total loss:  0.44589126110076904\n",
      "total loss:  0.3751794993877411\n",
      "total loss:  0.4374465048313141\n",
      "total loss:  0.4267745018005371\n",
      "total loss:  0.44823014736175537\n",
      "total loss:  0.45109114050865173\n",
      "total loss:  0.4216739237308502\n",
      "total loss:  0.4023663401603699\n",
      "total loss:  0.44785699248313904\n",
      "total loss:  0.39121150970458984\n",
      "total loss:  0.45418262481689453\n",
      "total loss:  0.3934963643550873\n",
      "total loss:  0.36279067397117615\n",
      "total loss:  0.41432663798332214\n",
      "total loss:  0.4166894257068634\n",
      "total loss:  0.44157299399375916\n",
      "total loss:  0.42102745175361633\n",
      "total loss:  0.3657761812210083\n",
      "total loss:  0.37630000710487366\n",
      "total loss:  0.3989616334438324\n",
      "total loss:  0.4218370318412781\n",
      "total loss:  0.3799091875553131\n",
      "Starting epoch 10\n",
      "total loss:  0.3928989768028259\n",
      "total loss:  0.4306071698665619\n",
      "total loss:  0.37428542971611023\n",
      "total loss:  0.4046582579612732\n",
      "total loss:  0.3930363953113556\n",
      "total loss:  0.4146212339401245\n",
      "total loss:  0.40501925349235535\n",
      "total loss:  0.3959665298461914\n",
      "total loss:  0.4535827040672302\n",
      "total loss:  0.3831818103790283\n",
      "total loss:  0.38939544558525085\n",
      "total loss:  0.40192681550979614\n",
      "total loss:  0.4012333154678345\n",
      "total loss:  0.43913519382476807\n",
      "total loss:  0.33909696340560913\n",
      "total loss:  0.4581812620162964\n",
      "total loss:  0.3705218732357025\n",
      "total loss:  0.39860987663269043\n",
      "total loss:  0.4204494059085846\n",
      "total loss:  0.396077036857605\n",
      "total loss:  0.37705495953559875\n",
      "total loss:  0.4774738848209381\n",
      "total loss:  0.4253409504890442\n",
      "total loss:  0.3832225501537323\n",
      "total loss:  0.32077470421791077\n",
      "total loss:  0.39856451749801636\n",
      "total loss:  0.3679068386554718\n",
      "total loss:  0.3580312430858612\n",
      "total loss:  0.4011881947517395\n",
      "total loss:  0.44665056467056274\n",
      "total loss:  0.44174060225486755\n",
      "total loss:  0.41644299030303955\n",
      "total loss:  0.39863675832748413\n",
      "total loss:  0.3616016209125519\n",
      "total loss:  0.38028284907341003\n",
      "total loss:  0.35883843898773193\n",
      "total loss:  0.42330700159072876\n",
      "total loss:  0.4483453035354614\n",
      "total loss:  0.4027872085571289\n",
      "total loss:  0.5461126565933228\n",
      "[[411.  54.]\n",
      " [ 17.  18.]]\n",
      "Iteration 400. Validation Loss 0.3735893666744232. Validation Accuracy 85. Validation Precision 51.42857142857143. Validation Recall 25.0. Validation F1 33.64485981308411. Validation class-wise F1 62.84706596477273.\n",
      "[[835. 103.]\n",
      " [ 15.  47.]]\n",
      "Iteration 400. Training Loss 0.316756010055542. Training Accuracy 88. Training Precision 75.80645161290323. Training Recall 31.333333333333332. Training F1 44.339622641509436. Training class-wise F1 68.87003503440125.\n",
      "Starting epoch 11\n",
      "total loss:  0.4306607246398926\n",
      "total loss:  0.41234663128852844\n",
      "total loss:  0.42910274863243103\n",
      "total loss:  0.33490076661109924\n",
      "total loss:  0.35619547963142395\n",
      "total loss:  0.44660651683807373\n",
      "total loss:  0.38467738032341003\n",
      "total loss:  0.3843788504600525\n",
      "total loss:  0.36141446232795715\n",
      "total loss:  0.42196205258369446\n",
      "total loss:  0.41447532176971436\n",
      "total loss:  0.40775221586227417\n",
      "total loss:  0.397895872592926\n",
      "total loss:  0.40319138765335083\n",
      "total loss:  0.3694738447666168\n",
      "total loss:  0.3575103282928467\n",
      "total loss:  0.3380943536758423\n",
      "total loss:  0.3867682218551636\n",
      "total loss:  0.39360806345939636\n",
      "total loss:  0.412971168756485\n",
      "total loss:  0.416311115026474\n",
      "total loss:  0.37257155776023865\n",
      "total loss:  0.41559720039367676\n",
      "total loss:  0.36291030049324036\n",
      "total loss:  0.38497671484947205\n",
      "total loss:  0.416156530380249\n",
      "total loss:  0.38313865661621094\n",
      "total loss:  0.42345502972602844\n",
      "total loss:  0.33166226744651794\n",
      "total loss:  0.37613150477409363\n",
      "total loss:  0.42881450057029724\n",
      "total loss:  0.3356793224811554\n",
      "total loss:  0.3680170476436615\n",
      "total loss:  0.4225687086582184\n",
      "total loss:  0.381475031375885\n",
      "total loss:  0.3408505916595459\n",
      "total loss:  0.4589643180370331\n",
      "total loss:  0.3976898193359375\n",
      "total loss:  0.388927161693573\n",
      "total loss:  0.41580647230148315\n",
      "Starting epoch 12\n",
      "total loss:  0.4023401141166687\n",
      "total loss:  0.3848501145839691\n",
      "total loss:  0.4925593137741089\n",
      "total loss:  0.36020955443382263\n",
      "total loss:  0.40577736496925354\n",
      "total loss:  0.36347365379333496\n",
      "total loss:  0.35623985528945923\n",
      "total loss:  0.41369980573654175\n",
      "total loss:  0.33884531259536743\n",
      "total loss:  0.45734909176826477\n",
      "total loss:  0.3582213819026947\n",
      "total loss:  0.36707645654678345\n",
      "total loss:  0.37042829394340515\n",
      "total loss:  0.32923567295074463\n",
      "total loss:  0.37253355979919434\n",
      "total loss:  0.379961222410202\n",
      "total loss:  0.36962005496025085\n",
      "total loss:  0.31813374161720276\n",
      "total loss:  0.41420137882232666\n",
      "total loss:  0.3461795151233673\n",
      "total loss:  0.36596348881721497\n",
      "total loss:  0.43766286969184875\n",
      "total loss:  0.43107184767723083\n",
      "total loss:  0.3708522617816925\n",
      "total loss:  0.3778218626976013\n",
      "total loss:  0.4016043543815613\n",
      "total loss:  0.40139031410217285\n",
      "total loss:  0.4223156273365021\n",
      "total loss:  0.4031672179698944\n",
      "total loss:  0.2950141429901123\n",
      "total loss:  0.386191725730896\n",
      "total loss:  0.38345542550086975\n",
      "total loss:  0.4455564022064209\n",
      "total loss:  0.3610324561595917\n",
      "total loss:  0.4103540778160095\n",
      "total loss:  0.37651875615119934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  0.36421188712120056\n",
      "total loss:  0.3467249274253845\n",
      "total loss:  0.34895768761634827\n",
      "total loss:  0.31050053238868713\n",
      "Starting epoch 13\n",
      "total loss:  0.47825345396995544\n",
      "total loss:  0.3853483498096466\n",
      "total loss:  0.3416697084903717\n",
      "total loss:  0.36173513531684875\n",
      "total loss:  0.4382287263870239\n",
      "total loss:  0.3412337303161621\n",
      "total loss:  0.42932453751564026\n",
      "total loss:  0.3676883280277252\n",
      "total loss:  0.3714163899421692\n",
      "total loss:  0.36065933108329773\n",
      "total loss:  0.33759644627571106\n",
      "total loss:  0.35438454151153564\n",
      "total loss:  0.37266990542411804\n",
      "total loss:  0.38675832748413086\n",
      "total loss:  0.39574408531188965\n",
      "total loss:  0.48632025718688965\n",
      "total loss:  0.3944985270500183\n",
      "total loss:  0.43802931904792786\n",
      "total loss:  0.3245357275009155\n",
      "total loss:  0.33483439683914185\n",
      "total loss:  0.36064690351486206\n",
      "total loss:  0.38524603843688965\n",
      "total loss:  0.36329057812690735\n",
      "total loss:  0.3918398916721344\n",
      "total loss:  0.341764360666275\n",
      "total loss:  0.42477256059646606\n",
      "total loss:  0.3381735384464264\n",
      "total loss:  0.3635276257991791\n",
      "total loss:  0.3204366862773895\n",
      "total loss:  0.3462966978549957\n",
      "total loss:  0.3742564022541046\n",
      "total loss:  0.4092283546924591\n",
      "total loss:  0.3904717266559601\n",
      "total loss:  0.3380718231201172\n",
      "total loss:  0.3925071954727173\n",
      "total loss:  0.38934844732284546\n",
      "total loss:  0.37412840127944946\n",
      "total loss:  0.34541046619415283\n",
      "total loss:  0.3838018476963043\n",
      "total loss:  0.23100125789642334\n",
      "Starting epoch 14\n",
      "total loss:  0.39732033014297485\n",
      "total loss:  0.3928093910217285\n",
      "total loss:  0.3234686553478241\n",
      "total loss:  0.38780033588409424\n",
      "total loss:  0.40983450412750244\n",
      "total loss:  0.37731656432151794\n",
      "total loss:  0.39661678671836853\n",
      "total loss:  0.3456666171550751\n",
      "total loss:  0.37509167194366455\n",
      "total loss:  0.27515652775764465\n",
      "total loss:  0.34261980652809143\n",
      "total loss:  0.38770583271980286\n",
      "total loss:  0.451450914144516\n",
      "total loss:  0.33051592111587524\n",
      "total loss:  0.3392082452774048\n",
      "total loss:  0.3474785387516022\n",
      "total loss:  0.3740038275718689\n",
      "total loss:  0.2989814281463623\n",
      "total loss:  0.34228330850601196\n",
      "total loss:  0.3718354403972626\n",
      "total loss:  0.3605034053325653\n",
      "total loss:  0.4151899814605713\n",
      "total loss:  0.3724513053894043\n",
      "total loss:  0.3640877604484558\n",
      "total loss:  0.33631086349487305\n",
      "total loss:  0.3887259364128113\n",
      "total loss:  0.3372538685798645\n",
      "total loss:  0.4148327112197876\n",
      "total loss:  0.3922500014305115\n",
      "total loss:  0.378164142370224\n",
      "total loss:  0.42874667048454285\n",
      "total loss:  0.372185617685318\n",
      "total loss:  0.31603124737739563\n",
      "total loss:  0.33887290954589844\n",
      "total loss:  0.3423898220062256\n",
      "total loss:  0.4457048177719116\n",
      "total loss:  0.4042055606842041\n",
      "total loss:  0.3681603968143463\n",
      "total loss:  0.40418338775634766\n",
      "total loss:  0.5119508504867554\n",
      "Starting epoch 15\n",
      "total loss:  0.32343170046806335\n",
      "total loss:  0.3601912260055542\n",
      "total loss:  0.3435244858264923\n",
      "total loss:  0.3474080264568329\n",
      "total loss:  0.3893985450267792\n",
      "total loss:  0.29587650299072266\n",
      "total loss:  0.3218229115009308\n",
      "total loss:  0.39690786600112915\n",
      "total loss:  0.28892073035240173\n",
      "total loss:  0.3601495325565338\n",
      "total loss:  0.44916006922721863\n",
      "total loss:  0.4036632776260376\n",
      "total loss:  0.42510148882865906\n",
      "total loss:  0.3852676749229431\n",
      "total loss:  0.33844250440597534\n",
      "total loss:  0.3984750509262085\n",
      "total loss:  0.31394729018211365\n",
      "total loss:  0.41009601950645447\n",
      "total loss:  0.39682045578956604\n",
      "total loss:  0.37443023920059204\n",
      "total loss:  0.34549614787101746\n",
      "total loss:  0.361057311296463\n",
      "total loss:  0.3332243263721466\n",
      "total loss:  0.3531647324562073\n",
      "total loss:  0.3476563096046448\n",
      "total loss:  0.4121251702308655\n",
      "total loss:  0.4301876723766327\n",
      "total loss:  0.4215552806854248\n",
      "total loss:  0.4091048538684845\n",
      "total loss:  0.35308530926704407\n",
      "total loss:  0.3435039222240448\n",
      "total loss:  0.35337406396865845\n",
      "total loss:  0.437903493642807\n",
      "total loss:  0.35278093814849854\n",
      "total loss:  0.3662935793399811\n",
      "total loss:  0.33707666397094727\n",
      "total loss:  0.33632412552833557\n",
      "total loss:  0.36237725615501404\n",
      "total loss:  0.28840041160583496\n",
      "total loss:  0.3929940462112427\n",
      "[[418.  54.]\n",
      " [ 10.  18.]]\n",
      "Iteration 600. Validation Loss 0.3528176248073578. Validation Accuracy 87. Validation Precision 64.28571428571429. Validation Recall 25.0. Validation F1 36.0. Validation class-wise F1 64.44444444444444.\n",
      "[[840. 103.]\n",
      " [ 10.  47.]]\n",
      "Iteration 600. Training Loss 0.28837260603904724. Training Accuracy 88. Training Precision 82.45614035087719. Training Recall 31.333333333333332. Training F1 45.410628019323674. Training class-wise F1 69.55417067446942.\n",
      "Starting epoch 16\n",
      "total loss:  0.3554796576499939\n",
      "total loss:  0.4162016212940216\n",
      "total loss:  0.33733561635017395\n",
      "total loss:  0.3473891615867615\n",
      "total loss:  0.3721475303173065\n",
      "total loss:  0.408449649810791\n",
      "total loss:  0.38726550340652466\n",
      "total loss:  0.39742371439933777\n",
      "total loss:  0.3518763780593872\n",
      "total loss:  0.4058404564857483\n",
      "total loss:  0.3370589315891266\n",
      "total loss:  0.37038561701774597\n",
      "total loss:  0.3354780077934265\n",
      "total loss:  0.38715723156929016\n",
      "total loss:  0.31881844997406006\n",
      "total loss:  0.38343968987464905\n",
      "total loss:  0.31031402945518494\n",
      "total loss:  0.3649195730686188\n",
      "total loss:  0.39824801683425903\n",
      "total loss:  0.30690833926200867\n",
      "total loss:  0.4123663008213043\n",
      "total loss:  0.3373258113861084\n",
      "total loss:  0.34606310725212097\n",
      "total loss:  0.3840346932411194\n",
      "total loss:  0.3571968376636505\n",
      "total loss:  0.3241885304450989\n",
      "total loss:  0.33258846402168274\n",
      "total loss:  0.3324018120765686\n",
      "total loss:  0.33967912197113037\n",
      "total loss:  0.36283838748931885\n",
      "total loss:  0.3859398663043976\n",
      "total loss:  0.39070406556129456\n",
      "total loss:  0.3311057984828949\n",
      "total loss:  0.3944386839866638\n",
      "total loss:  0.34537214040756226\n",
      "total loss:  0.3281187415122986\n",
      "total loss:  0.3899911046028137\n",
      "total loss:  0.41702988743782043\n",
      "total loss:  0.3518892526626587\n",
      "total loss:  0.2245156466960907\n",
      "Starting epoch 17\n",
      "total loss:  0.3480755388736725\n",
      "total loss:  0.31990471482276917\n",
      "total loss:  0.3575718402862549\n",
      "total loss:  0.41412609815597534\n",
      "total loss:  0.311830997467041\n",
      "total loss:  0.32847869396209717\n",
      "total loss:  0.3400469422340393\n",
      "total loss:  0.36119237542152405\n",
      "total loss:  0.378915399312973\n",
      "total loss:  0.3173620104789734\n",
      "total loss:  0.31585314869880676\n",
      "total loss:  0.40058183670043945\n",
      "total loss:  0.3991175889968872\n",
      "total loss:  0.4188600778579712\n",
      "total loss:  0.42309892177581787\n",
      "total loss:  0.33732935786247253\n",
      "total loss:  0.35805416107177734\n",
      "total loss:  0.38791510462760925\n",
      "total loss:  0.3685806095600128\n",
      "total loss:  0.33343198895454407\n",
      "total loss:  0.3113333284854889\n",
      "total loss:  0.367482453584671\n",
      "total loss:  0.34200480580329895\n",
      "total loss:  0.3549686074256897\n",
      "total loss:  0.33460721373558044\n",
      "total loss:  0.3196447193622589\n",
      "total loss:  0.321025550365448\n",
      "total loss:  0.3257870078086853\n",
      "total loss:  0.36226850748062134\n",
      "total loss:  0.33610156178474426\n",
      "total loss:  0.39751389622688293\n",
      "total loss:  0.36776214838027954\n",
      "total loss:  0.36075255274772644\n",
      "total loss:  0.35736024379730225\n",
      "total loss:  0.4193804860115051\n",
      "total loss:  0.35249826312065125\n",
      "total loss:  0.4345667362213135\n",
      "total loss:  0.3155350685119629\n",
      "total loss:  0.3553505837917328\n",
      "total loss:  0.3269754648208618\n",
      "Starting epoch 18\n",
      "total loss:  0.3139446973800659\n",
      "total loss:  0.3986206352710724\n",
      "total loss:  0.38035860657691956\n",
      "total loss:  0.38178586959838867\n",
      "total loss:  0.32136380672454834\n",
      "total loss:  0.3793944716453552\n",
      "total loss:  0.31456848978996277\n",
      "total loss:  0.37143415212631226\n",
      "total loss:  0.35797828435897827\n",
      "total loss:  0.3696860074996948\n",
      "total loss:  0.3737136423587799\n",
      "total loss:  0.31378936767578125\n",
      "total loss:  0.3440400958061218\n",
      "total loss:  0.29881682991981506\n",
      "total loss:  0.34245190024375916\n",
      "total loss:  0.37000125646591187\n",
      "total loss:  0.37116384506225586\n",
      "total loss:  0.4204061031341553\n",
      "total loss:  0.33841025829315186\n",
      "total loss:  0.3153436779975891\n",
      "total loss:  0.3458317518234253\n",
      "total loss:  0.4030766785144806\n",
      "total loss:  0.3136284649372101\n",
      "total loss:  0.3529851734638214\n",
      "total loss:  0.4011366665363312\n",
      "total loss:  0.38129591941833496\n",
      "total loss:  0.3789120018482208\n",
      "total loss:  0.3194033205509186\n",
      "total loss:  0.31729066371917725\n",
      "total loss:  0.33333468437194824\n",
      "total loss:  0.3953061103820801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  0.3364924490451813\n",
      "total loss:  0.34360817074775696\n",
      "total loss:  0.2919561564922333\n",
      "total loss:  0.33342689275741577\n",
      "total loss:  0.3926818072795868\n",
      "total loss:  0.3645480275154114\n",
      "total loss:  0.3275393545627594\n",
      "total loss:  0.3383967876434326\n",
      "total loss:  0.39076852798461914\n",
      "Starting epoch 19\n",
      "total loss:  0.3273644745349884\n",
      "total loss:  0.34806230664253235\n",
      "total loss:  0.3353683352470398\n",
      "total loss:  0.3497403562068939\n",
      "total loss:  0.38346871733665466\n",
      "total loss:  0.4113236963748932\n",
      "total loss:  0.4232126772403717\n",
      "total loss:  0.3207990825176239\n",
      "total loss:  0.3746936023235321\n",
      "total loss:  0.34528598189353943\n",
      "total loss:  0.298734188079834\n",
      "total loss:  0.3482123911380768\n",
      "total loss:  0.402088463306427\n",
      "total loss:  0.3942941427230835\n",
      "total loss:  0.32977211475372314\n",
      "total loss:  0.3484899401664734\n",
      "total loss:  0.34188854694366455\n",
      "total loss:  0.3226921558380127\n",
      "total loss:  0.2907339334487915\n",
      "total loss:  0.33833256363868713\n",
      "total loss:  0.3646685779094696\n",
      "total loss:  0.3480471968650818\n",
      "total loss:  0.3310691714286804\n",
      "total loss:  0.31150445342063904\n",
      "total loss:  0.36426177620887756\n",
      "total loss:  0.42528074979782104\n",
      "total loss:  0.353008508682251\n",
      "total loss:  0.3524181842803955\n",
      "total loss:  0.31994712352752686\n",
      "total loss:  0.3468872904777527\n",
      "total loss:  0.28594112396240234\n",
      "total loss:  0.32517191767692566\n",
      "total loss:  0.3906976878643036\n",
      "total loss:  0.32103365659713745\n",
      "total loss:  0.3729414641857147\n",
      "total loss:  0.39193886518478394\n",
      "total loss:  0.3013901114463806\n",
      "total loss:  0.366880863904953\n",
      "total loss:  0.2950381934642792\n",
      "total loss:  0.4858620762825012\n",
      "Starting epoch 20\n",
      "total loss:  0.35362276434898376\n",
      "total loss:  0.33293625712394714\n",
      "total loss:  0.3611584007740021\n",
      "total loss:  0.2921004891395569\n",
      "total loss:  0.329532265663147\n",
      "total loss:  0.3810085356235504\n",
      "total loss:  0.3082568645477295\n",
      "total loss:  0.4223391115665436\n",
      "total loss:  0.2888871431350708\n",
      "total loss:  0.3522191047668457\n",
      "total loss:  0.33893251419067383\n",
      "total loss:  0.36974218487739563\n",
      "total loss:  0.3408912420272827\n",
      "total loss:  0.32617709040641785\n",
      "total loss:  0.3471178114414215\n",
      "total loss:  0.3202168643474579\n",
      "total loss:  0.34608757495880127\n",
      "total loss:  0.2958321273326874\n",
      "total loss:  0.3802229166030884\n",
      "total loss:  0.3512589931488037\n",
      "total loss:  0.36072561144828796\n",
      "total loss:  0.368218332529068\n",
      "total loss:  0.3785886764526367\n",
      "total loss:  0.29045242071151733\n",
      "total loss:  0.3566974699497223\n",
      "total loss:  0.351345419883728\n",
      "total loss:  0.31091809272766113\n",
      "total loss:  0.32391321659088135\n",
      "total loss:  0.3382203280925751\n",
      "total loss:  0.4754275381565094\n",
      "total loss:  0.3223767876625061\n",
      "total loss:  0.33410558104515076\n",
      "total loss:  0.4053327739238739\n",
      "total loss:  0.3180941939353943\n",
      "total loss:  0.3008202016353607\n",
      "total loss:  0.3489983379840851\n",
      "total loss:  0.37366005778312683\n",
      "total loss:  0.35734158754348755\n",
      "total loss:  0.3247990012168884\n",
      "total loss:  0.13035674393177032\n",
      "[[417.  52.]\n",
      " [ 11.  20.]]\n",
      "Iteration 800. Validation Loss 0.3493257761001587. Validation Accuracy 87. Validation Precision 64.51612903225806. Validation Recall 27.77777777777778. Validation F1 38.83495145631068. Validation class-wise F1 65.9057700425366.\n",
      "[[835.  87.]\n",
      " [ 15.  63.]]\n",
      "Iteration 800. Training Loss 0.2710612118244171. Training Accuracy 89. Training Precision 80.76923076923077. Training Recall 42.0. Training F1 55.26315789473684. Training class-wise F1 74.75347510989664.\n",
      "Starting epoch 21\n",
      "total loss:  0.32130351662635803\n",
      "total loss:  0.2950899600982666\n",
      "total loss:  0.32593289017677307\n",
      "total loss:  0.3138863444328308\n",
      "total loss:  0.3267683982849121\n",
      "total loss:  0.41303491592407227\n",
      "total loss:  0.328536719083786\n",
      "total loss:  0.3942810893058777\n",
      "total loss:  0.35708609223365784\n",
      "total loss:  0.33374524116516113\n",
      "total loss:  0.35899391770362854\n",
      "total loss:  0.28768280148506165\n",
      "total loss:  0.3995867371559143\n",
      "total loss:  0.306762158870697\n",
      "total loss:  0.31777217984199524\n",
      "total loss:  0.33336538076400757\n",
      "total loss:  0.3226010203361511\n",
      "total loss:  0.3597571849822998\n",
      "total loss:  0.3343570828437805\n",
      "total loss:  0.4004252254962921\n",
      "total loss:  0.3203831911087036\n",
      "total loss:  0.380761057138443\n",
      "total loss:  0.3529658913612366\n",
      "total loss:  0.36751657724380493\n",
      "total loss:  0.3541536033153534\n",
      "total loss:  0.36958855390548706\n",
      "total loss:  0.36250266432762146\n",
      "total loss:  0.35484394431114197\n",
      "total loss:  0.36098968982696533\n",
      "total loss:  0.3380885124206543\n",
      "total loss:  0.3373449742794037\n",
      "total loss:  0.35451725125312805\n",
      "total loss:  0.33505597710609436\n",
      "total loss:  0.30899566411972046\n",
      "total loss:  0.39348000288009644\n",
      "total loss:  0.27802330255508423\n",
      "total loss:  0.2974020838737488\n",
      "total loss:  0.38125497102737427\n",
      "total loss:  0.364128053188324\n",
      "total loss:  0.36312586069107056\n",
      "Starting epoch 22\n",
      "total loss:  0.3551015853881836\n",
      "total loss:  0.3452639579772949\n",
      "total loss:  0.31057482957839966\n",
      "total loss:  0.33449268341064453\n",
      "total loss:  0.3562929034233093\n",
      "total loss:  0.31077876687049866\n",
      "total loss:  0.366761714220047\n",
      "total loss:  0.3598288297653198\n",
      "total loss:  0.3591616749763489\n",
      "total loss:  0.30186134576797485\n",
      "total loss:  0.30613547563552856\n",
      "total loss:  0.35873493552207947\n",
      "total loss:  0.29541251063346863\n",
      "total loss:  0.34154215455055237\n",
      "total loss:  0.36147987842559814\n",
      "total loss:  0.33866769075393677\n",
      "total loss:  0.3673107624053955\n",
      "total loss:  0.3463757336139679\n",
      "total loss:  0.3084092438220978\n",
      "total loss:  0.31448256969451904\n",
      "total loss:  0.3572063148021698\n",
      "total loss:  0.35729271173477173\n",
      "total loss:  0.3856131136417389\n",
      "total loss:  0.35832715034484863\n",
      "total loss:  0.31972184777259827\n",
      "total loss:  0.3440573513507843\n",
      "total loss:  0.2748759686946869\n",
      "total loss:  0.3355909585952759\n",
      "total loss:  0.35985150933265686\n",
      "total loss:  0.4096345603466034\n",
      "total loss:  0.3526884913444519\n",
      "total loss:  0.3703591525554657\n",
      "total loss:  0.31402748823165894\n",
      "total loss:  0.3218296766281128\n",
      "total loss:  0.30905652046203613\n",
      "total loss:  0.35171958804130554\n",
      "total loss:  0.3130989968776703\n",
      "total loss:  0.31198543310165405\n",
      "total loss:  0.3557664453983307\n",
      "total loss:  0.18288123607635498\n",
      "Starting epoch 23\n",
      "total loss:  0.35787954926490784\n",
      "total loss:  0.3111865520477295\n",
      "total loss:  0.34574881196022034\n",
      "total loss:  0.36771172285079956\n",
      "total loss:  0.3325991630554199\n",
      "total loss:  0.3561096489429474\n",
      "total loss:  0.3488680124282837\n",
      "total loss:  0.3214649260044098\n",
      "total loss:  0.3462775647640228\n",
      "total loss:  0.31137171387672424\n",
      "total loss:  0.2913493514060974\n",
      "total loss:  0.3644470274448395\n",
      "total loss:  0.3707199990749359\n",
      "total loss:  0.35913175344467163\n",
      "total loss:  0.36683356761932373\n",
      "total loss:  0.3317349851131439\n",
      "total loss:  0.3764670491218567\n",
      "total loss:  0.31688961386680603\n",
      "total loss:  0.32507607340812683\n",
      "total loss:  0.31588244438171387\n",
      "total loss:  0.2656354010105133\n",
      "total loss:  0.30898141860961914\n",
      "total loss:  0.2959270477294922\n",
      "total loss:  0.3581385314464569\n",
      "total loss:  0.3867020606994629\n",
      "total loss:  0.33061864972114563\n",
      "total loss:  0.28740426898002625\n",
      "total loss:  0.3719373047351837\n",
      "total loss:  0.3539280891418457\n",
      "total loss:  0.37071746587753296\n",
      "total loss:  0.3701368570327759\n",
      "total loss:  0.34120428562164307\n",
      "total loss:  0.2820875644683838\n",
      "total loss:  0.3103666603565216\n",
      "total loss:  0.340422123670578\n",
      "total loss:  0.29701900482177734\n",
      "total loss:  0.3651532828807831\n",
      "total loss:  0.30600959062576294\n",
      "total loss:  0.32292962074279785\n",
      "total loss:  0.27742835879325867\n",
      "Starting epoch 24\n",
      "total loss:  0.31432589888572693\n",
      "total loss:  0.40178823471069336\n",
      "total loss:  0.37134072184562683\n",
      "total loss:  0.31390419602394104\n",
      "total loss:  0.34385839104652405\n",
      "total loss:  0.39513859152793884\n",
      "total loss:  0.2753881812095642\n",
      "total loss:  0.34837663173675537\n",
      "total loss:  0.3564211428165436\n",
      "total loss:  0.40168529748916626\n",
      "total loss:  0.337660014629364\n",
      "total loss:  0.37091153860092163\n",
      "total loss:  0.3448111414909363\n",
      "total loss:  0.3217373788356781\n",
      "total loss:  0.250638872385025\n",
      "total loss:  0.30887719988822937\n",
      "total loss:  0.33595678210258484\n",
      "total loss:  0.3742956221103668\n",
      "total loss:  0.3094438910484314\n",
      "total loss:  0.37712109088897705\n",
      "total loss:  0.3264240026473999\n",
      "total loss:  0.30753055214881897\n",
      "total loss:  0.34641966223716736\n",
      "total loss:  0.3246576189994812\n",
      "total loss:  0.3594968616962433\n",
      "total loss:  0.331118106842041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  0.34678947925567627\n",
      "total loss:  0.352632075548172\n",
      "total loss:  0.312369704246521\n",
      "total loss:  0.3254830539226532\n",
      "total loss:  0.29758307337760925\n",
      "total loss:  0.337289959192276\n",
      "total loss:  0.28563976287841797\n",
      "total loss:  0.3784399628639221\n",
      "total loss:  0.28337982296943665\n",
      "total loss:  0.30815795063972473\n",
      "total loss:  0.3138113021850586\n",
      "total loss:  0.24873630702495575\n",
      "total loss:  0.36038917303085327\n",
      "total loss:  0.24870306253433228\n",
      "Starting epoch 25\n",
      "total loss:  0.3264745771884918\n",
      "total loss:  0.30586329102516174\n",
      "total loss:  0.30149713158607483\n",
      "total loss:  0.3607213497161865\n",
      "total loss:  0.3510645031929016\n",
      "total loss:  0.37391453981399536\n",
      "total loss:  0.28370392322540283\n",
      "total loss:  0.33049294352531433\n",
      "total loss:  0.33412429690361023\n",
      "total loss:  0.36165496706962585\n",
      "total loss:  0.25702473521232605\n",
      "total loss:  0.3361806273460388\n",
      "total loss:  0.3173214793205261\n",
      "total loss:  0.3657014071941376\n",
      "total loss:  0.26762670278549194\n",
      "total loss:  0.29500266909599304\n",
      "total loss:  0.38712045550346375\n",
      "total loss:  0.3598887622356415\n",
      "total loss:  0.30142101645469666\n",
      "total loss:  0.35597389936447144\n",
      "total loss:  0.34289875626564026\n",
      "total loss:  0.3458949625492096\n",
      "total loss:  0.3333134353160858\n",
      "total loss:  0.3405672013759613\n",
      "total loss:  0.3286437392234802\n",
      "total loss:  0.3219998776912689\n",
      "total loss:  0.369479775428772\n",
      "total loss:  0.3364410996437073\n",
      "total loss:  0.3276616632938385\n",
      "total loss:  0.34635406732559204\n",
      "total loss:  0.3123764097690582\n",
      "total loss:  0.37808069586753845\n",
      "total loss:  0.3147800862789154\n",
      "total loss:  0.3221302330493927\n",
      "total loss:  0.3677195906639099\n",
      "total loss:  0.33289575576782227\n",
      "total loss:  0.3220856487751007\n",
      "total loss:  0.28008097410202026\n",
      "total loss:  0.3109440803527832\n",
      "total loss:  0.30902087688446045\n",
      "[[412.  49.]\n",
      " [ 16.  23.]]\n",
      "Iteration 1000. Validation Loss 0.34873589873313904. Validation Accuracy 87. Validation Precision 58.97435897435897. Validation Recall 31.944444444444443. Validation F1 41.44144144144144. Validation class-wise F1 67.06492769484895.\n",
      "[[834.  82.]\n",
      " [ 16.  68.]]\n",
      "Iteration 1000. Training Loss 0.256625771522522. Training Accuracy 90. Training Precision 80.95238095238095. Training Recall 45.333333333333336. Training F1 58.119658119658126. Training class-wise F1 76.28519712324922.\n",
      "Starting epoch 26\n",
      "total loss:  0.35094964504241943\n",
      "total loss:  0.3418184518814087\n",
      "total loss:  0.34996849298477173\n",
      "total loss:  0.3169543445110321\n",
      "total loss:  0.31354254484176636\n",
      "total loss:  0.33507558703422546\n",
      "total loss:  0.29285773634910583\n",
      "total loss:  0.34883230924606323\n",
      "total loss:  0.33329105377197266\n",
      "total loss:  0.3271925449371338\n",
      "total loss:  0.30982303619384766\n",
      "total loss:  0.26929327845573425\n",
      "total loss:  0.29223865270614624\n",
      "total loss:  0.27471861243247986\n",
      "total loss:  0.3177564740180969\n",
      "total loss:  0.3506585359573364\n",
      "total loss:  0.2867600917816162\n",
      "total loss:  0.386283814907074\n",
      "total loss:  0.31606802344322205\n",
      "total loss:  0.27930983901023865\n",
      "total loss:  0.30922114849090576\n",
      "total loss:  0.383512943983078\n",
      "total loss:  0.33836615085601807\n",
      "total loss:  0.3140106201171875\n",
      "total loss:  0.3497037887573242\n",
      "total loss:  0.33303946256637573\n",
      "total loss:  0.2817884087562561\n",
      "total loss:  0.3656314015388489\n",
      "total loss:  0.3622276186943054\n",
      "total loss:  0.32436826825141907\n",
      "total loss:  0.27901220321655273\n",
      "total loss:  0.3417466878890991\n",
      "total loss:  0.307140588760376\n",
      "total loss:  0.3700558841228485\n",
      "total loss:  0.26575401425361633\n",
      "total loss:  0.36884889006614685\n",
      "total loss:  0.28027331829071045\n",
      "total loss:  0.36943113803863525\n",
      "total loss:  0.3269023597240448\n",
      "total loss:  0.3138989210128784\n",
      "Starting epoch 27\n",
      "total loss:  0.3255068361759186\n",
      "total loss:  0.3152443468570709\n",
      "total loss:  0.35443082451820374\n",
      "total loss:  0.38517093658447266\n",
      "total loss:  0.3330119550228119\n",
      "total loss:  0.27019384503364563\n",
      "total loss:  0.37773969769477844\n",
      "total loss:  0.3504377007484436\n",
      "total loss:  0.2813572287559509\n",
      "total loss:  0.32328277826309204\n",
      "total loss:  0.35020673274993896\n",
      "total loss:  0.3146078884601593\n",
      "total loss:  0.31816747784614563\n",
      "total loss:  0.32002243399620056\n",
      "total loss:  0.3267503082752228\n",
      "total loss:  0.3047187626361847\n",
      "total loss:  0.3599061071872711\n",
      "total loss:  0.34973400831222534\n",
      "total loss:  0.31812167167663574\n",
      "total loss:  0.31641197204589844\n",
      "total loss:  0.2865164875984192\n",
      "total loss:  0.3271116614341736\n",
      "total loss:  0.30829212069511414\n",
      "total loss:  0.3811146318912506\n",
      "total loss:  0.3368053436279297\n",
      "total loss:  0.29738861322402954\n",
      "total loss:  0.2850557267665863\n",
      "total loss:  0.3010871410369873\n",
      "total loss:  0.3261297047138214\n",
      "total loss:  0.29699090123176575\n",
      "total loss:  0.3106791079044342\n",
      "total loss:  0.31704574823379517\n",
      "total loss:  0.27035093307495117\n",
      "total loss:  0.36135318875312805\n",
      "total loss:  0.3328481614589691\n",
      "total loss:  0.29370298981666565\n",
      "total loss:  0.3433528542518616\n",
      "total loss:  0.32581621408462524\n",
      "total loss:  0.2764012813568115\n",
      "total loss:  0.24840548634529114\n",
      "Starting epoch 28\n",
      "total loss:  0.269639790058136\n",
      "total loss:  0.3648489713668823\n",
      "total loss:  0.3164157569408417\n",
      "total loss:  0.33193156123161316\n",
      "total loss:  0.3004075586795807\n",
      "total loss:  0.3497813642024994\n",
      "total loss:  0.362591028213501\n",
      "total loss:  0.34176135063171387\n",
      "total loss:  0.2840832769870758\n",
      "total loss:  0.313787043094635\n",
      "total loss:  0.3510711193084717\n",
      "total loss:  0.33471500873565674\n",
      "total loss:  0.33933568000793457\n",
      "total loss:  0.31898602843284607\n",
      "total loss:  0.35874447226524353\n",
      "total loss:  0.30655649304389954\n",
      "total loss:  0.2836676836013794\n",
      "total loss:  0.29559391736984253\n",
      "total loss:  0.32800430059432983\n",
      "total loss:  0.28700771927833557\n",
      "total loss:  0.365911066532135\n",
      "total loss:  0.3242000639438629\n",
      "total loss:  0.3807082772254944\n",
      "total loss:  0.2938331067562103\n",
      "total loss:  0.2893691956996918\n",
      "total loss:  0.30456361174583435\n",
      "total loss:  0.3290550708770752\n",
      "total loss:  0.33990001678466797\n",
      "total loss:  0.317076176404953\n",
      "total loss:  0.39729925990104675\n",
      "total loss:  0.2941135764122009\n",
      "total loss:  0.3223808705806732\n",
      "total loss:  0.3356100022792816\n",
      "total loss:  0.291757732629776\n",
      "total loss:  0.31767693161964417\n",
      "total loss:  0.2882991135120392\n",
      "total loss:  0.3708430230617523\n",
      "total loss:  0.32072538137435913\n",
      "total loss:  0.3354853391647339\n",
      "total loss:  0.3422219753265381\n",
      "Starting epoch 29\n",
      "total loss:  0.3217064142227173\n",
      "total loss:  0.24864986538887024\n",
      "total loss:  0.3880077004432678\n",
      "total loss:  0.3383038640022278\n",
      "total loss:  0.29487869143486023\n",
      "total loss:  0.30763015151023865\n",
      "total loss:  0.36938098073005676\n",
      "total loss:  0.3307071924209595\n",
      "total loss:  0.33529430627822876\n",
      "total loss:  0.3693884015083313\n",
      "total loss:  0.3510071933269501\n",
      "total loss:  0.32987648248672485\n",
      "total loss:  0.3095798194408417\n",
      "total loss:  0.34098049998283386\n",
      "total loss:  0.34100738167762756\n",
      "total loss:  0.3385032117366791\n",
      "total loss:  0.316934734582901\n",
      "total loss:  0.31404179334640503\n",
      "total loss:  0.26736557483673096\n",
      "total loss:  0.3658175468444824\n",
      "total loss:  0.24075636267662048\n",
      "total loss:  0.33398348093032837\n",
      "total loss:  0.32041823863983154\n",
      "total loss:  0.33607032895088196\n",
      "total loss:  0.24246785044670105\n",
      "total loss:  0.32644322514533997\n",
      "total loss:  0.3314371407032013\n",
      "total loss:  0.2558606266975403\n",
      "total loss:  0.31521713733673096\n",
      "total loss:  0.332660049200058\n",
      "total loss:  0.3335284888744354\n",
      "total loss:  0.2521001696586609\n",
      "total loss:  0.3481768071651459\n",
      "total loss:  0.3091784715652466\n",
      "total loss:  0.3246169984340668\n",
      "total loss:  0.3271039128303528\n",
      "total loss:  0.29337459802627563\n",
      "total loss:  0.2632949650287628\n",
      "total loss:  0.3295815587043762\n",
      "total loss:  0.629691481590271\n",
      "Starting epoch 30\n",
      "total loss:  0.3336648941040039\n",
      "total loss:  0.35548946261405945\n",
      "total loss:  0.305123507976532\n",
      "total loss:  0.3515397608280182\n",
      "total loss:  0.2699640393257141\n",
      "total loss:  0.3325386941432953\n",
      "total loss:  0.2989634871482849\n",
      "total loss:  0.337717741727829\n",
      "total loss:  0.31458231806755066\n",
      "total loss:  0.30784380435943604\n",
      "total loss:  0.3006046414375305\n",
      "total loss:  0.3256268799304962\n",
      "total loss:  0.3516499400138855\n",
      "total loss:  0.3251894414424896\n",
      "total loss:  0.279193252325058\n",
      "total loss:  0.3065432012081146\n",
      "total loss:  0.27220261096954346\n",
      "total loss:  0.27974286675453186\n",
      "total loss:  0.393278568983078\n",
      "total loss:  0.302253782749176\n",
      "total loss:  0.2845473289489746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  0.27039840817451477\n",
      "total loss:  0.3783615827560425\n",
      "total loss:  0.27377039194107056\n",
      "total loss:  0.31124576926231384\n",
      "total loss:  0.3325096368789673\n",
      "total loss:  0.27841004729270935\n",
      "total loss:  0.34164518117904663\n",
      "total loss:  0.2667475938796997\n",
      "total loss:  0.297016441822052\n",
      "total loss:  0.2506733536720276\n",
      "total loss:  0.2858319878578186\n",
      "total loss:  0.29213061928749084\n",
      "total loss:  0.34355905652046204\n",
      "total loss:  0.29343724250793457\n",
      "total loss:  0.33977144956588745\n",
      "total loss:  0.27718207240104675\n",
      "total loss:  0.35578614473342896\n",
      "total loss:  0.39988765120506287\n",
      "total loss:  0.360710084438324\n",
      "[[408.  47.]\n",
      " [ 20.  25.]]\n",
      "Iteration 1200. Validation Loss 0.3522036671638489. Validation Accuracy 86. Validation Precision 55.55555555555556. Validation Recall 34.72222222222222. Validation F1 42.73504273504274. Validation class-wise F1 67.57363688281015.\n",
      "[[832.  76.]\n",
      " [ 18.  74.]]\n",
      "Iteration 1200. Training Loss 0.24644902348518372. Training Accuracy 90. Training Precision 80.43478260869566. Training Recall 49.333333333333336. Training F1 61.15702479338843. Training class-wise F1 77.90501979146099.\n",
      "Starting epoch 31\n",
      "total loss:  0.3267146646976471\n",
      "total loss:  0.3313302993774414\n",
      "total loss:  0.3213500380516052\n",
      "total loss:  0.27579358220100403\n",
      "total loss:  0.3166772723197937\n",
      "total loss:  0.27758437395095825\n",
      "total loss:  0.2924947738647461\n",
      "total loss:  0.32868483662605286\n",
      "total loss:  0.3400750458240509\n",
      "total loss:  0.36234942078590393\n",
      "total loss:  0.2559804320335388\n",
      "total loss:  0.3424464762210846\n",
      "total loss:  0.35927021503448486\n",
      "total loss:  0.210086390376091\n",
      "total loss:  0.31527024507522583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b31cd9e097bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mrnn_clf_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mrnn_clf_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mnum_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# embedded_train = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_train), example[2]]\n",
    "#                       for example in raw_train[:10000]]\n",
    "# train_dataset = TextDataset([example[0] for example in embedded_train],\n",
    "#                                 [example[1] for example in embedded_train])\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#                                   collate_fn=TextDataset.collate_fn)\n",
    "\n",
    "# embedded_train_small = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_train), example[2]]\n",
    "#                       for example in raw_train[:1000]]\n",
    "# train_dataset_small = TextDataset([example[0] for example in embedded_train_small],\n",
    "#                                 [example[1] for example in embedded_train_small])\n",
    "# train_dataloader_small = DataLoader(dataset=train_dataset_small, batch_size=batch_size, shuffle=True,\n",
    "#                                   collate_fn=TextDataset.collate_fn)\n",
    "\n",
    "# embedded_dev_small = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_dev), example[2]]\n",
    "#                       for example in raw_dev[:500]]\n",
    "# dev_dataset_small = TextDataset([example[0] for example in embedded_dev],\n",
    "#                                 [example[1] for example in embedded_dev])\n",
    "# dev_dataloader_small = DataLoader(dataset=dev_dataset, batch_size=batch_size, shuffle=True,\n",
    "#                                   collate_fn=TextDataset.collate_fn)\n",
    "\n",
    "using_GPU = False\n",
    "\"\"\"\n",
    "3. Model training\n",
    "\"\"\"\n",
    "'''\n",
    "3. 1 \n",
    "set up model, loss criterion, optimizer\n",
    "'''\n",
    "# Instantiate the model\n",
    "# embedding_dim = glove + elmo + suffix indicator\n",
    "# dropout1: dropout on input to RNN\n",
    "# dropout2: dropout in RNN; would not be used if num_layers=1\n",
    "# dropout3: dropout on hidden state of RNN to linear layer\n",
    "# Separate the input (embedded_sequence) and labels in the indexed train sets.\n",
    "\n",
    "# change embedding_dim=300+1024+50 if with elmo + suffix, 512 or 300 for hidden_size\n",
    "rnn_clf = RNNSequenceClassifier(num_classes=2, embedding_dim=300, hidden_size=100, num_layers=1, bidir=True,\n",
    "                                dropout1=0, dropout2=0.2, dropout3=0)\n",
    "# Move the model to the GPU if available\n",
    "if using_GPU:\n",
    "    rnn_clf = rnn_clf.cuda()\n",
    "# Set up criterion for calculating loss\n",
    "nll_criterion = nn.NLLLoss()\n",
    "# Set up an optimizer for updating the parameters of the rnn_clf\n",
    "# rnn_clf_optimizer = optim.SGD(rnn_clf.parameters(), lr=0.01, momentum=0.9)\n",
    "rnn_clf_optimizer = optim.Adam(rnn_clf.parameters(), lr=0.0001)\n",
    "# Number of epochs (passes through the dataset) to train the model for.\n",
    "num_epochs = 100\n",
    "\n",
    "'''\n",
    "3. 2\n",
    "train model\n",
    "'''\n",
    "# without checkpoint, print during training\n",
    "all_train_loss = [] \n",
    "# with checkpoints, print and plot at the end\n",
    "training_loss = []\n",
    "val_loss = []\n",
    "training_f1 = []\n",
    "val_f1 = []\n",
    "# A counter for the number of gradient updates\n",
    "num_iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Starting epoch {}\".format(epoch + 1))\n",
    "    for (example_text, example_lengths, labels) in train_dataloader:\n",
    "        example_text = Variable(example_text)\n",
    "        example_lengths = Variable(example_lengths)\n",
    "        labels = Variable(labels)\n",
    "        if using_GPU:\n",
    "            example_text = example_text.cuda()\n",
    "            example_lengths = example_lengths.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # predicted shape: (batch_size, 2)\n",
    "        predicted = rnn_clf(example_text, example_lengths)\n",
    "        batch_loss = nll_criterion(predicted, labels)\n",
    "        rnn_clf_optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        rnn_clf_optimizer.step()\n",
    "        # keep record\n",
    "        num_iter += 1\n",
    "        all_train_loss.append(batch_loss.item())\n",
    "        print('total loss: ', batch_loss.item())\n",
    "        # Calculate validation and training set loss and accuracy every 200 gradient updates\n",
    "        if num_iter % 200 == 0:\n",
    "            avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(dev_dataloader_small, rnn_clf,\n",
    "                                                                                   nll_criterion, using_GPU)\n",
    "            val_loss.append(avg_eval_loss)\n",
    "            val_f1.append(f1)\n",
    "            print(\n",
    "                \"Iteration {}. Validation Loss {}. Validation Accuracy {}. Validation Precision {}. Validation Recall {}. Validation F1 {}. Validation class-wise F1 {}.\".format(\n",
    "                    num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))\n",
    "            # filename = '../models/LSTMSuffixElmoAtt_???_all_iter_' + str(num_iter) + '.pt'\n",
    "            # torch.save(rnn_clf, filename)\n",
    "#             avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(train_dataloader_small, rnn_clf,\n",
    "#                                                                                    nll_criterion, using_GPU)\n",
    "#             training_loss.append(avg_eval_loss)\n",
    "#             training_f1.append(f1)\n",
    "#             print(\n",
    "#                 \"Iteration {}. Training Loss {}. Training Accuracy {}. Training Precision {}. Training Recall {}. Training F1 {}. Training class-wise F1 {}.\".format(\n",
    "#                     num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))\n",
    "print(\"Training done!\")\n",
    "\n",
    "# cannot display the graph in terminal on remote server\n",
    "\"\"\"\n",
    "3.3\n",
    "plot the training process: MET F1 and losses for validation and training dataset\n",
    "\"\"\"\n",
    "plt.figure(0)\n",
    "plt.title('F1 for VUA dataset')\n",
    "plt.xlabel('iteration (unit:200)')\n",
    "plt.ylabel('F1')\n",
    "plt.plot(val_f1,'g')\n",
    "plt.plot(training_f1, 'b')\n",
    "plt.legend(['Validation F1', 'Training F1'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Loss for VUA dataset')\n",
    "plt.xlabel('iteration (unit:200)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(val_loss,'g')\n",
    "plt.plot(training_loss, 'b')\n",
    "plt.legend(['Validation loss', 'Training loss'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11052.  1599.]\n",
      " [  672.  1340.]]\n",
      "Iteration 1215. Validation Loss 0.39598435163497925. Validation Accuracy 84. Validation Precision 66.60039761431412. Validation Recall 45.593739367131676. Validation F1 54.1304786911735. Validation class-wise F1 72.40677780712521.\n"
     ]
    }
   ],
   "source": [
    "embedded_dev = [[embed_sequence(example[0], word2idx, glove_embeddings, elmos_dev), example[2]]\n",
    "                      for example in raw_dev]\n",
    "dev_dataset = TextDataset([example[0] for example in embedded_dev],\n",
    "                                [example[1] for example in embedded_dev])\n",
    "dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                  collate_fn=TextDataset.collate_fn)\n",
    "avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(dev_dataloader, rnn_clf,\n",
    "                                                                       nll_criterion, using_GPU)\n",
    "val_loss.append(avg_eval_loss)\n",
    "val_f1.append(f1)\n",
    "print(\n",
    "    \"Iteration {}. Validation Loss {}. Validation Accuracy {}. Validation Precision {}. Validation Recall {}. Validation F1 {}. Validation class-wise F1 {}.\".format(\n",
    "        num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
